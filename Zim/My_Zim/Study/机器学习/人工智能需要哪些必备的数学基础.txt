Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2017-12-11T06:32:36+08:00

====== 人工智能需要哪些必备的数学基础 ======
Created 星期一 11 十二月 2017

当下，人工智能成了新时代的必修课，其重要性已无需赘述，但作为一个跨学科产物，它包含的内容浩如烟海，各种复杂的模型和算法更是让人望而生畏。对于大多数的新手来说，如何入手人工智能其实都是一头雾水，比如到底需要哪些数学基础、是否要有工程经验、对于深度学习框架应该关注什么等等。

那么，学习人工智能该从哪里开始呢？人工智能的学习路径又是怎样的？
本文节选自王天一教授在极客时间 App 开设的“人工智能基础课”，已获授权。更多相关文章，请下载极客时间 App，订阅专栏获取。
数学基础知识蕴含着处理智能问题的基本思想与方法，也是理解复杂算法的必备要素。今天的种种人工智能技术归根到底都建立在数学模型之上，要了解人工智能，首先要掌握必备的数学基础知识，具体来说包括：
线性代数：如何将研究对象形式化？
概率论：如何描述统计规律？
数理统计：如何以小见大？
最优化理论： 如何找到最优解？
信息论：如何定量度量不确定性？
形式逻辑：如何实现抽象推理？

线性代数：如何将研究对象形式化？
事实上，线性代数不仅仅是人工智能的基础，更是现代数学和以现代数学作为主要分析方法的众多学科的基础。从量子力学到图像处理都离不开向量和矩阵的使用。而在向量和矩阵背后，线性代数的核心意义在于提供了⼀种看待世界的抽象视角：万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察。
着重于抽象概念的解释而非具体的数学公式来看，线性代数要点如下：线性代数的本质在于将具体事物抽象为数学对象，并描述其静态和动态的特性；向量的实质是 n 维线性空间中的静止点；线性变换描述了向量或者作为参考系的坐标系的变化，可以用矩阵表示；矩阵的特征值和特征向量描述了变化的速度与方向。
总之，线性代数之于人工智能如同加法之于高等数学，是一个基础的工具集。

概率论：如何描述统计规律？
除了线性代数之外，概率论也是人工智能研究中必备的数学基础。随着连接主义学派的兴起，概率统计已经取代了数理逻辑，成为人工智能研究的主流工具。在数据爆炸式增长和计算力指数化增强的今天，概率论已经在机器学习中扮演了核心角色。
同线性代数一样，概率论也代表了一种看待世界的方式，其关注的焦点是无处不在的可能性。频率学派认为先验分布是固定的，模型参数要靠最大似然估计计算；贝叶斯学派认为先验分布是随机的，模型参数要靠后验概率最大化计算；正态分布是最重要的一种随机变量的分布。

数理统计：如何以小见大？
在人工智能的研究中，数理统计同样不可或缺。基础的统计理论有助于对机器学习的算法和数据挖掘的结果做出解释，只有做出合理的解读，数据的价值才能够体现。数理统计根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断。
虽然数理统计以概率论为理论基础，但两者之间存在方法上的本质区别。概率论作用的前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征与规律；数理统计的研究对象则是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，根据得到的观察结果对原始分布做出推断。
用一句不严谨但直观的话讲：数理统计可以看成是逆向的概率论。 数理统计的任务是根据可观察的样本反过来推断总体的性质；推断的工具是统计量，统计量是样本的函数，是个随机变量；参数估计通过随机抽取的样本来估计总体分布的未知参数，包括点估计和区间估计；假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用于估计机器学习模型的泛化错误率。

最优化理论： 如何找到最优解？
本质上讲，人工智能的目标就是最优化：在复杂环境与多体交互中做出最优决策。几乎所有的人工智能问题最后都会归结为一个优化问题的求解，因而最优化理论同样是人工智能必备的基础知识。最优化理论研究的问题是判定给定目标函数的最大值（最小值）是否存在，并找到令目标函数取到最大值 (最小值) 的数值。 如果把给定的目标函数看成一座山脉，最优化的过程就是判断顶峰的位置并找到到达顶峰路径的过程。
通常情况下，最优化问题是在无约束情况下求解给定目标函数的最小值；在线性搜索中，确定寻找最小值时的搜索方向需要使用目标函数的一阶导数和二阶导数；置信域算法的思想是先确定搜索步长，再确定搜索方向；以人工神经网络为代表的启发式算法是另外一类重要的优化方法。

信息论：如何定量度量不确定性？
近年来的科学研究不断证实，不确定性就是客观世界的本质属性。换句话说，上帝还真就掷骰子。不确定性的世界只能使用概率模型来描述，这促成了信息论的诞生。
信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。
总之，信息论处理的是客观世界中的不确定性；条件熵和信息增益是分类问题中的重要参数；KL 散度用于描述两个不同概率分布之间的差异；最大熵原理是分类问题汇总的常用准则。

形式逻辑：如何实现抽象推理？
1956 年召开的达特茅斯会议宣告了人工智能的诞生。在人工智能的襁褓期，各位奠基者们，包括约翰·麦卡锡、赫伯特·西蒙、马文·闵斯基等未来的图灵奖得主，他们的愿景是让“具备抽象思考能力的程序解释合成的物质如何能够拥有人类的心智。”通俗地说，理想的人工智能应该具有抽象意义上的学习、推理与归纳能力，其通用性将远远强于解决国际象棋或是围棋等具体问题的算法。
如果将认知过程定义为对符号的逻辑运算，人工智能的基础就是形式逻辑；谓词逻辑是知识表示的主要方法；基于谓词逻辑系统可以实现具有自动推理能力的人工智能；不完备性定理向“认知的本质是计算”这一人工智能的基本理念提出挑战。
《人工智能基础课》全年目录
本专栏将围绕机器学习与神经网络等核心概念展开，并结合当下火热的深度学习技术，勾勒出人工智能发展的基本轮廓与主要路径。

专栏订阅指南
1. 请在苹果和各大安卓应用市场搜索下载“极客时间”App，注册登录；
2. 在发现页找到专栏入口，点击专栏完成购买。

作者：AI前线
链接：https://juejin.im/post/5a2bd31a51882540f36379d0
来源：掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


作者：AI前线
链接：https://juejin.im/post/5a2bd31a51882540f36379d0
来源：掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

