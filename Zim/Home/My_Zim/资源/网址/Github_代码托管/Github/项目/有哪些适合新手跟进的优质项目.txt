Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2018-12-06T16:45:28+08:00

====== 有哪些适合新手跟进的优质项目 ======
Created Thursday 06 December 2018

https://www.zhihu.com/question/22744854/answer/452012367?utm_source=com.youdao.note&utm_medium=social&utm_oi=33829276352512

这个项目在 GitHub 上叫 textgenrnn。它是一款基于 Keras/TensorFlow 的 Python 3 模块，可以用来创建字符级的循环神经网络。它有以下几个亮点：
能够训练或生成字符级和词汇级的文本。
能够配置 RNN 大小、RNN 层级数量，决定是否用双向 RNN。
能够用任何输入文本文件训练，包括大型文件。
能够在 GPU 上训练模型，然后在 CPU 上用它们生成文本。
能够用上下文信息标签训练模型，使模型更快的生成更好的结果。

在这个 Colaboratory Book（https://drive.google.com/file/d/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK/view%EF%BC%89%E4%B8%8A%EF%BC%8C%E4%BD%A0%E5%8F%AF%E4%BB%A5%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AAGPU%EF%BC%8C%E7%94%A8textgenrnn%E8%AE%AD%E7%BB%83%E4%BB%BB%E4%BD%95%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6。

示例

from textgenrnn import textgenrnn

textgen = textgenrnn()
textgen.generate()

[Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard.


使用新的文本就能很容易的训练内置的模型，哪怕输入数据只传入了一次，模型也能生成恰当的文本。

textgen.train_from_file('hacker-news-2000.txt', num_epochs=1)
textgen.generate()

Project State Project Firefox


模型权重很小（硬盘 2M），可以很容易的保存和加载入新的 textgenrnn 实例。

textgen_2 = textgenrnn('/weights/hacker_news.hdf5')
textgen_2.generate(3, temperature=1.0)

Why we got money “regular alter”

Urburg to Firefox acquires Nelf Multi Shamn

Kubernetes by Google’s Bern


用法

可以通过 pip 从 pypi 处安装 textgenrnn：

pip3 install textgenrnn


你可以从这个 Jupyter Notebook 上查看常见特征和模型配置选项的演示：

https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb

/datasets 包含用于训练 textgenrnn 的来自 Hacker News/Reddit 的样本数据集。
/weights 包含用前面提到的能够加载入 textgenrnn 的数据集所训练的预训练模型。
/outputs 包含上面的预训练模型生成的文本样例。

textgenrnn 架构与实现

Textgenrnn 基于人工智能大神 Andrej Karpathy 研发的 char-rnn 项目，并略作了改进，比如能够处理非常小的文本序列。


注意：
生成的文本不会非常完美，即便是使用了训练程度非常高的神经网络。
用不同的数据集生成的结果会相差很大。因为预训练的神经网络相对较小，没法和 RNN 存储一样多的数据。最好使用至少有 2000-5000 文件的数据集。如果数据集很小，需要延长训练时间，将 num_epochs 参数调高。
再训练 textgenrnn 并不需要 GPU，但是在 CPU 上训练时间会长很多。如果你使用 GPU，建议增加 batch_size 参数的值，以更好的利用硬件。

未来优化计划
推出更多的正式文档。
增加监督式文本生成模式。
能让模型架构用于聊天机器人的聊天。
增加能调整更长字符序列、更深入理解语言、生成更好的文本的预训练神经网络。

Textgenrnn 在 Github 上地址：

https://github.com/minimaxir/textgenrnn

--------------------------------------------------------------------------------
参考资料：

https://github.com/minimaxir/textgenrnn

http://aiweirdness.com/post/173797162852/ai-scream-for-ice-cream

